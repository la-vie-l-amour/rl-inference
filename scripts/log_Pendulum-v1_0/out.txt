16:52:34

=== Loading experiment [device: cuda] ===

{'action_noise': None,
 'action_repeat': 3,
 'batch_size': 50,
 'coverage': False,
 'ensemble_size': 10,
 'env_name': 'Pendulum-v1',
 'epsilon': 1e-08,
 'expl_scale': 1.0,
 'expl_strategy': 'information',
 'grad_clip_norm': 1000,
 'hidden_size': 64,
 'learning_rate': 0.001,
 'logdir': 'Pendulum-v1',
 'max_episode_len': 100,
 'n_candidates': 500,
 'n_episodes': 5,
 'n_seed_episodes': 5,
 'n_train_epochs': 100,
 'optimisation_iters': 5,
 'plan_horizon': 5,
 'record_every': 0,
 'reward_scale': 1.0,
 'seed': 0,
 'strategy': 'information',
 'top_candidates': 50,
 'use_exploration': True,
 'use_mean': False,
 'use_reward': True}

Collected seeds: [5 episodes | 335 frames]

=== Episode 1 ===
Training on [335/1005] data points
> Train epoch 20 [ensemble 27.50 | reward 276.57]
> Train epoch 40 [ensemble 3.40 | reward 153.58]
> Train epoch 60 [ensemble -8.27 | reward 104.16]
> Train epoch 80 [ensemble -15.84 | reward 78.95]
> Train epoch 100 [ensemble -20.90 | reward 63.64]
Ensemble loss -20.90 / Reward Loss 63.64
Setup recoder @ log_Pendulum-v1_0/videos/1.mp4

=== Collecting data [1] ===
> Step 25 [reward -129.67]
> Step 50 [reward -135.84]
Rewards -139.38 / Steps 67.00
Reward stats:
 {'max': '-0.84', 'mean': '-25.18', 'min': '-119.16', 'std': '20.59'}
Information gain stats:
 {'max': '19.56', 'mean': '10.18', 'min': '-0.32', 'std': '2.14'}
Episode time 8.42
Saved _metrics_

=== Episode 2 ===
Training on [402/1206] data points
> Train epoch 20 [ensemble 19.72 | reward 189.93]
> Train epoch 40 [ensemble -1.63 | reward 101.57]
> Train epoch 60 [ensemble -12.71 | reward 68.68]
> Train epoch 80 [ensemble -20.07 | reward 52.01]
> Train epoch 100 [ensemble -25.13 | reward 41.89]
Ensemble loss -25.13 / Reward Loss 41.89
Setup recoder @ log_Pendulum-v1_0/videos/2.mp4

=== Collecting data [2] ===
> Step 25 [reward -124.09]
> Step 50 [reward -124.66]
Rewards -124.92 / Steps 67.00
Reward stats:
 {'max': '1.60', 'mean': '-8.16', 'min': '-142.56', 'std': '21.24'}
Information gain stats:
 {'max': '38.88', 'mean': '5.49', 'min': '-1.35', 'std': '2.11'}
Episode time 5.74
Saved _metrics_

=== Episode 3 ===
Training on [469/1407] data points
> Train epoch 20 [ensemble 16.04 | reward 151.32]
> Train epoch 40 [ensemble -2.87 | reward 79.74]
> Train epoch 60 [ensemble -13.30 | reward 54.13]
> Train epoch 80 [ensemble -20.80 | reward 41.14]
> Train epoch 100 [ensemble -25.74 | reward 33.24]
Ensemble loss -25.74 / Reward Loss 33.24
Setup recoder @ log_Pendulum-v1_0/videos/3.mp4

=== Collecting data [3] ===
> Step 25 [reward -122.89]
> Step 50 [reward -123.05]
Rewards -123.14 / Steps 67.00
Reward stats:
 {'max': '1.11', 'mean': '-11.29', 'min': '-117.12', 'std': '23.10'}
Information gain stats:
 {'max': '14.29', 'mean': '4.99', 'min': '-1.84', 'std': '1.54'}
Episode time 6.49
Saved _metrics_

=== Episode 4 ===
Training on [536/1608] data points
> Train epoch 20 [ensemble 14.44 | reward 104.84]
> Train epoch 40 [ensemble -4.54 | reward 54.31]
> Train epoch 60 [ensemble -14.87 | reward 36.89]
> Train epoch 80 [ensemble -22.12 | reward 27.98]
> Train epoch 100 [ensemble -26.91 | reward 22.56]
Ensemble loss -26.91 / Reward Loss 22.56
Setup recoder @ log_Pendulum-v1_0/videos/4.mp4

=== Collecting data [4] ===
> Step 25 [reward -237.62]
> Step 50 [reward -238.01]
Rewards -238.21 / Steps 67.00
Reward stats:
 {'max': '1.40', 'mean': '-17.89', 'min': '-136.13', 'std': '31.29'}
Information gain stats:
 {'max': '17.36', 'mean': '4.61', 'min': '-2.15', 'std': '1.51'}
Episode time 6.95
Saved _metrics_